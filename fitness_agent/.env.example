# ── Model Provider ─────────────────────────────────────────
# Options: "gemini" (default) or "ollama"
MODEL_PROVIDER=gemini

# ── Gemini Config (when MODEL_PROVIDER=gemini) ────────────
GOOGLE_API_KEY=your_google_api_key_here
GEMINI_MODEL=gemini-2.5-flash

# ── Ollama Config (when MODEL_PROVIDER=ollama) ────────────
# Make sure Ollama is running: ollama serve
# And you've pulled a model: ollama pull llama3.1:8b
OLLAMA_MODEL=llama3.1:8b
OLLAMA_BASE_URL=http://localhost:11434
